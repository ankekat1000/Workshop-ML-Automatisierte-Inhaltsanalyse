{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methoden-Workshop Machine Learning & AIA\n",
    "1. **Inhaltsanalyse-Daten einlesen uns ansehen** mit der Python-Bibliothek für Data Management ``pandas``.\n",
    "2. **Text in Features umwandeln** - mit ``sklearn``'s ``Vectorizer``.\n",
    "3. **Classification Functions** auswählen und anwenden.\n",
    "4. Datensatz in **Train und Test Set** aufteilen.\n",
    "5. Die Funktion auf den Trainingsdaten **fitten** aka. **trainieren**.\n",
    "6. **Evaluieren** - Die Predictions mit der (manuellen) Codierung auf dem **Test Set abgleichen**.\n",
    "\n",
    "Von: [anke.stoll@hhu.de](mailto:anke.stoll@hhu.de) <br>\n",
    "Last edit: 08.02.2021\n",
    "\n",
    "\n",
    "Basic Tutorials:\n",
    "\n",
    "- [Python Basics Tutorial](https://www.youtube.com/watch?v=eXBD2bB9-RA&list=PLQVvvaa0QuDeAams7fkdcwOGBpGdHpXln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Von Excel nach Python und wieder zurück mit ``pandas``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Abkürzung für Faule und aus Konvention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daten einlesen - .csv or .txt oder Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wo liegt/wie heißt der Datensatz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data Sets/HateSpeech_Tweets_DataSet.csv\",\n",
    "                 sep=\",\") #Ändern bei Excel auf \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check out your data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10) #Zeigt die ersten 10 Zeilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10) #Oder die letzen 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df) #Anzahl der Zeilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HateSpeech\"].value_counts() #Überblick zu den Werten in einer Spalte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt noch viel mehr Funktionen und Möglichkeiten, Daten mit der `pandas`-Bibliothek zu bearbeiten, zu analysieren und darzustellen! \n",
    "\n",
    "Diese beiden **Playlists auf Youtube** sind sehr gut, vor allem für Menschen ohne Informatik-Vorkenntnisse:\n",
    "\n",
    "- [Data School Pandas Best Practice](https://www.youtube.com/watch?v=hl-TGI4550M&list=PL5-da3qGB5IBITZj_dYSFqnd_15JgqwA6)\n",
    "- [Pandas and Data Science](https://www.youtube.com/watch?v=yzIMircGU5I&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y) Schon etwas älter, inzwischen gibt es 1,2 kleine Änderungen in der Bibliothek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text zu Features - Unabhängigen Variablen programmieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die zweite super awesome Python Bibliothek heißt ``sklearn``. In dieser Bibliothek finden wir alles, was wir für ML brauchen. Auch das ML mit Text wird uns besonders einfach gemacht. In ``sklearn`` gibt es eine Unterabteilung ``feature_extraction.text``. Hier finden wir Funktionen, die uns das Umwandeln von Text zu Wort-Vektoren sehr einfach machen! Wir benutzen ``CountVectorizer`` and ``TfidfVectorizer``. Full documentation auf der [sklearn Website](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "\n",
    "- [Machine Learning mit sklearn](https://www.youtube.com/watch?v=elojMnjn4kk&list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A) -Schon älter, aber immer noch eins der besten Tutorials die es gibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All vectorizers have several parameters that you can modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1,1)) \n",
    "#Ich speicher den hier ab, damit ich nicht immer ausschreiben muss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_vec = vec.fit_transform(df[\"Tweet\"]) \n",
    "#Wir wenden den Vectorizer auf der Spalte \"Tweet\" in unserem Datensatz an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_vec # 469 tweets und 2645 features (unigrams)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Was sind unsere Features (UVs)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names() #Das sind unsere Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used [this code from stuckoverflow](https://stackoverflow.com/questions/45805493/sorting-tfidfvectorizer-output-by-tf-idf-lowest-to-highest-and-vice-versa). Alternatively, you can try [this code](https://towardsdatascience.com/very-simple-python-script-for-extracting-most-common-words-from-a-story-1e3570d0b9d0) to get an overview of the most frequent words in your documents (before vectorization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = Tweets_vec.sum(axis=0) \n",
    "\n",
    "data = []\n",
    "\n",
    "for col, term in enumerate(features):\n",
    "    data.append( (term, sums[0,col] ))\n",
    "\n",
    "df_ranks = pd.DataFrame(data, columns=['Feature','Count'])\n",
    "df_ranks.sort_values('Count', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranks.tail(20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jetzt Bigrams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(2,2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_vec = vec.fit_transform(df[\"Tweet\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names() #Das sind unsere Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Den Classifier programmieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir jetzt nun Features (UVs, Wörter mit ihren Häufigkeiten) erstellt, als AV haben wir Hate Speech, YES or NO. Jetzt berechnen wir einen Zusammenhang, heißt, wir versuchen die Kategorie _Hate Speech_ durch unsere Feautures zu zu schätzen (klassifizieren)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als erstes müssen wir uns für eine Schätzfunktion - eine _Classification Function_ - entscheiden. Und zwar aus der Auswahl, die in der `sklearn` zu finden ist. Hier eine kleine Auswahl von Funktionen, die sich in der Forschung als geeignet herausgestellt haben. Spolier: Meistens muss man durch Trail und Error gerausfinden, welche sich eignet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Den Datensatz in Train und Test Set aufteilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =df[\"HateSpeech\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, #X is your pandas column with the documents.\n",
    "                                                    y, #y is your pandas coulmn with category.\n",
    "                                                    test_size=0.33,#Relative size of the test set. e.g. 33% \n",
    "                                                    random_state=42) #Chose a number to reproduce the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train) #Anzahl der Tweets in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = vec.fit_transform(X_train) #Do this ONLY ON X TRAIN !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_vec, y_train) #Fit the model, what means training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vec = vec.transform(X_test)# Mache Predictions auf dem Test Set für die Evaluation. \n",
    "# Das Test Set muss auch i Vectorformat transformiert werden. Benutze die funtion transform, nicht fit_transform!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a nice overview of the performance of your model in all categories (on the test set).\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nice confusion matrix output as a pandas data frame.\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(y_test, y_pred),\n",
    "    columns=['Predicted NO', 'Predicted YES'],\n",
    "    index=['True NO', 'True YES']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fertig!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
